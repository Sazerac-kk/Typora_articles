# 深度学习系列——预备知识：线性回归



## 什么是线性回归？

线性回归是一种简单常用的有监督机器学习方法，虽然相对于其他回归分析方法，线性回归最为简单且乏味，但很多新学习方法仍以线性回归为基础，因此认证学习并掌握线性回归方法仍是很重要的

## 常见的回归分析方法

- 线性回归(Linear Regression)

- 逻辑回归（Logistic regressions）
- 多项式回归(Polynomial Regression)
- 逐步回归(Step Regression)
- 岭回归(Ridge Regression)
- 套索回归(Lasso Regression)
- 弹性网回归(ElasticNet)

## 简单线性回归模型

假设给定数据集如下图所示：

![d57c17088c56c184e88282fafc78e02](pictures/线性回归/d57c17088c56c184e88282fafc78e02.png)

一元线性回归就是找到一个函数$ f(x_i) = mx_i + b$ 使得对任意$ x_i $，有$ f(x_i) \approx y_i $ 。



我们尝试多种参数m和b的组合，的得到以上的图像，那么我们监督学习的过程就可以描述为：

  “给定N个数据对(x,y)，找出最合适的参数m*与b*，是模型能够更好得拟合这些数据。”（还是很易于理解的的~）

## 怎样衡量线性函数的好坏

上图给了很多条拟合直线，那么那一条是最符合要求的呢？人类有时可以用肉眼来分辨，但是计算机不行。实质上，计算机使用损失函数（loss function）来衡量这个问题，同时它又被称为代价函数（cost function）【PS：我比较喜欢使用cost function】。顾名思义，代价函数衡量了真实值$y$与预测值$\hat y$之间的差异程度，一般的代价函数表示如下：
$$
L(\widehat{y}, y)=\frac{1}{N} \sum_{i=1}^{N}\left(\widehat{y_{i}}-y_{i}\right)^{2}
$$
$L(\hat y , y)$表示所有误差的平均值。

那么为什么损失函数采用的是平方和而不是绝对值呢，这是由于平方和具有最小值存在，可利用求导的方式求取最小值，但是绝对值没有则个特性。

我们的任务：找出最合适的$m^*$和$b^*$ 
$$
m^*,b^*=\underset{m,b}{\arg\min}L(m,b)=\underset{m,b}{\arg\min}\frac{1}{N}\sum_{i=1}^N[(mx_i+b)-y_i]^2
$$




### 代码案例

```
import numpy as np 
import matplotlib.pyplot as plt  
from sklearn.linear_model import LinearRegression  # 线性回归
 
# 一、导入数据
# 样本数据集，第一列为x，第二列为y，在x和y之间建立回归模型
data=[
    [0.067732,3.176513],[0.427810,3.816464],[0.995731,4.550095],[0.738336,4.256571],[0.981083,4.560815],
    [0.526171,3.929515],[0.378887,3.526170],[0.033859,3.156393],[0.132791,3.110301],[0.138306,3.149813],
    [0.247809,3.476346],[0.648270,4.119688],[0.731209,4.282233],[0.236833,3.486582],[0.969788,4.655492],
    [0.607492,3.965162],[0.358622,3.514900],[0.147846,3.125947],[0.637820,4.094115],[0.230372,3.476039],
    [0.070237,3.210610],[0.067154,3.190612],[0.925577,4.631504],[0.717733,4.295890],[0.015371,3.085028],
    [0.335070,3.448080],[0.040486,3.167440],[0.212575,3.364266],[0.617218,3.993482],[0.541196,3.891471]
]
 
# 二、生成矩阵
#生成X和y矩阵
dataMat = np.array(data)
X = dataMat[:,0:1]   # 变量x
y = dataMat[:,1]   #变量y
 
 
# 三、线性回归
model = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
model.fit(X, y)   # 线性回归建模
print('系数矩阵:\n',model.coef_)
print('线性回归模型:\n',model)
# 使用模型预测
predicted = model.predict(X)
 
plt.scatter(X, y, marker='x')
plt.plot(X, predicted,c='r')
 
plt.xlabel("x")
plt.ylabel("y")
```

具体思路如下：

1. 首先从文件中导入数据并进行处理

2. 然后对列表进行切片，提取出X和y，可利用 `dataMat.shape`查看数据的格式，这里面返回结果应当是(30,2)

3. 最后进行线性回归，这里由于版本问题有一个错误，例如，你有可能在运行时出现

   1. ![image-20240916150611430](C:\Users\wangkaikai\AppData\Roaming\Typora\typora-user-images\image-20240916150611430.png)

   ​	这是由于在scikit-learn的0.22版本中，这个参数normalize被移除了，normalize的作用是对特征进行标准化。可以将其直接去掉。

   ​	

4. 讲解一下 `LinearRegression`的参数配置：

- **fit_intercept**：bool, default=True

  ​	是否计算此模型的截距。如果已设置 设置为 False，则计算中不会使用任何截距 （即数据应居中）。

- **copy_X**：bool, default=True

  ​	如果为 True，则将复制 X;否则，它可能会被覆盖。

- **n_jobs**：int, default=None

  ​	用于计算的作业数。这只会提供 speedup 在出现足够大的问题时，即 first 和 second 是稀疏的，或者如果设置了 自。 表示 1，除非在上下文中。 表示使用全部 处理器。有关更多详细信息，请参阅术语表 。n_targets > 1XpositiveTrueNone-1

- **positive**：bool, default=False

  ​	When set to , forces the coefficients to be positive. This option is only supported for dense arrays.`True`

  

